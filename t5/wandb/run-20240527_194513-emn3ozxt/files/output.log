2024-05-27 19:45:16 | INFO | fairseq.trainer | begin training epoch 1
2024-05-27 19:45:16 | INFO | fairseq_cli.train | Start iterating over samples
/fsx/homes/Hawau.Toyin@mbzuai.ac.ae/miniconda3/envs/fseq/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/fsx/homes/Hawau.Toyin@mbzuai.ac.ae/miniconda3/envs/fseq/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2024-05-27 19:47:15 | INFO | train_inner | {"epoch": 1, "update": 0.148, "s2t_loss": "9.892", "t2s_loss": "0.8661", "loss": "3.02666", "s2t_nll_loss": "2.544", "ctc_loss": "17.241", "ce_loss": "1.763", "s2t_accuracy": "49.05", "t2s_l1_loss": "0.78127", "t2s_l2_loss": "0.52926", "t2s_bce_loss": "0.08094", "s2t_total": "8415.31", "s2t_n_correct": "4127.73", "t2s_encoder_alpha": "0.12695", "t2s_decoder_alpha": "0.19051", "t2s_enc_dec_attn_loss": "0.00390496", "s2t_ppl": "5.83", "wps": "26542", "ups": "1.1", "wpb": "24196.3", "bsz": "220.3", "num_updates": "100", "lr": "1.264e-06", "gnorm": "2.233", "clip": "0", "loss_scale": "128", "train_wall": "92", "gb_free": "21.7", "wall": "133"}
2024-05-27 19:48:44 | INFO | train_inner | {"epoch": 1, "update": 0.296, "s2t_loss": "9.565", "t2s_loss": "0.70891", "loss": "2.79192", "s2t_nll_loss": "2.257", "ctc_loss": "16.873", "ce_loss": "1.564", "s2t_accuracy": "52.788", "t2s_l1_loss": "0.65772", "t2s_l2_loss": "0.39125", "t2s_bce_loss": "0.04731", "s2t_total": "8629.27", "s2t_n_correct": "4555.21", "t2s_encoder_alpha": "0.12695", "t2s_decoder_alpha": "0.19043", "t2s_enc_dec_attn_loss": "0.00387167", "s2t_ppl": "4.78", "wps": "27068.1", "ups": "1.12", "wpb": "24153.5", "bsz": "221.8", "num_updates": "200", "lr": "1.528e-06", "gnorm": "1.536", "clip": "0", "loss_scale": "128", "train_wall": "89", "gb_free": "29.5", "wall": "222"}
2024-05-27 19:50:15 | INFO | train_inner | {"epoch": 1, "update": 0.444, "s2t_loss": "8.588", "t2s_loss": "0.64224", "loss": "2.64407", "s2t_nll_loss": "2.285", "ctc_loss": "14.89", "ce_loss": "1.584", "s2t_accuracy": "51.772", "t2s_l1_loss": "0.60428", "t2s_l2_loss": "0.33276", "t2s_bce_loss": "0.03454", "s2t_total": "8711.79", "s2t_n_correct": "4510.22", "t2s_encoder_alpha": "0.12703", "t2s_decoder_alpha": "0.1903", "t2s_enc_dec_attn_loss": "0.00342274", "s2t_ppl": "4.87", "wps": "26765.2", "ups": "1.11", "wpb": "24177.8", "bsz": "210.6", "num_updates": "300", "lr": "1.792e-06", "gnorm": "2.668", "clip": "0", "loss_scale": "128", "train_wall": "90", "gb_free": "18.3", "wall": "313"}
2024-05-27 19:51:38 | INFO | train_inner | {"epoch": 1, "update": 0.592, "s2t_loss": "6.63", "t2s_loss": "0.61899", "loss": "1.89861", "s2t_nll_loss": "2.346", "ctc_loss": "10.915", "ce_loss": "1.626", "s2t_accuracy": "50.521", "t2s_l1_loss": "0.58554", "t2s_l2_loss": "0.31134", "t2s_bce_loss": "0.03017", "s2t_total": "7351.47", "s2t_n_correct": "3714.05", "t2s_encoder_alpha": "0.12708", "t2s_decoder_alpha": "0.19018", "t2s_enc_dec_attn_loss": "0.00329312", "s2t_ppl": "5.08", "wps": "29072.9", "ups": "1.2", "wpb": "24275.3", "bsz": "213.6", "num_updates": "400", "lr": "2.056e-06", "gnorm": "3.591", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "20.4", "wall": "396"}
2024-05-27 19:53:03 | INFO | train_inner | {"epoch": 1, "update": 0.74, "s2t_loss": "5.742", "t2s_loss": "0.60578", "loss": "1.77204", "s2t_nll_loss": "2.203", "ctc_loss": "9.281", "ce_loss": "1.527", "s2t_accuracy": "53.288", "t2s_l1_loss": "0.57491", "t2s_l2_loss": "0.30013", "t2s_bce_loss": "0.02763", "s2t_total": "8065.13", "s2t_n_correct": "4297.78", "t2s_encoder_alpha": "0.12712", "t2s_decoder_alpha": "0.19006", "t2s_enc_dec_attn_loss": "0.00323891", "s2t_ppl": "4.6", "wps": "28731.7", "ups": "1.19", "wpb": "24235.6", "bsz": "210.1", "num_updates": "500", "lr": "2.32e-06", "gnorm": "3.55", "clip": "0", "loss_scale": "128", "train_wall": "84", "gb_free": "28.8", "wall": "481"}
2024-05-27 19:54:28 | INFO | train_inner | {"epoch": 1, "update": 0.888, "s2t_loss": "5.241", "t2s_loss": "0.59645", "loss": "1.64448", "s2t_nll_loss": "2.045", "ctc_loss": "8.438", "ce_loss": "1.417", "s2t_accuracy": "56.024", "t2s_l1_loss": "0.56644", "t2s_l2_loss": "0.29079", "t2s_bce_loss": "0.0267", "s2t_total": "7877.19", "s2t_n_correct": "4413.11", "t2s_encoder_alpha": "0.1272", "t2s_decoder_alpha": "0.18994", "t2s_enc_dec_attn_loss": "0.00330421", "s2t_ppl": "4.13", "wps": "28230.4", "ups": "1.17", "wpb": "24177.6", "bsz": "210.9", "num_updates": "600", "lr": "2.584e-06", "gnorm": "3.275", "clip": "0", "loss_scale": "128", "train_wall": "85", "gb_free": "28.5", "wall": "566"}
2024-05-27 19:55:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 676 updates
2024-05-27 19:55:30 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint1.pt
2024-05-27 19:55:33 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint1.pt
2024-05-27 19:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint1.pt (epoch 1 @ 676 updates, score None) (writing took 5.772730725817382 seconds)
2024-05-27 19:55:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-05-27 19:55:36 | INFO | train | {"epoch": 1, "train_s2t_loss": "7.372", "train_t2s_loss": "0.66263", "train_loss": "2.21102", "train_s2t_nll_loss": "2.253", "train_ctc_loss": "12.491", "train_ce_loss": "1.562", "train_s2t_accuracy": "52.688", "train_t2s_l1_loss": "0.62004", "train_t2s_l2_loss": "0.35006", "train_t2s_bce_loss": "0.0391", "train_s2t_total": "8130.07", "train_s2t_n_correct": "4283.61", "train_t2s_encoder_alpha": "0.12707", "train_t2s_decoder_alpha": "0.19019", "train_t2s_enc_dec_attn_loss": "0.00348445", "train_s2t_ppl": "4.77", "train_wps": "27628.6", "train_ups": "1.14", "train_wpb": "24179.9", "train_bsz": "214.5", "train_num_updates": "676", "train_lr": "2.78464e-06", "train_gnorm": "2.825", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "584", "train_gb_free": "29.9", "train_wall": "634"}
2024-05-27 19:55:36 | INFO | fairseq.trainer | loading train data for epoch 2
2024-05-27 19:55:36 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 19:55:36 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 19:55:36 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 19:55:36 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 19:55:36 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 19:55:36 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 19:55:36 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 19:55:36 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 19:55:36 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 19:55:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 19:55:36 | INFO | fairseq.trainer | begin training epoch 2
2024-05-27 19:55:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 19:56:23 | INFO | train_inner | {"epoch": 2, "update": 1.036, "s2t_loss": "4.981", "t2s_loss": "0.59186", "loss": "1.52976", "s2t_nll_loss": "2.033", "ctc_loss": "7.93", "ce_loss": "1.409", "s2t_accuracy": "56.513", "t2s_l1_loss": "0.56331", "t2s_l2_loss": "0.28726", "t2s_bce_loss": "0.0252", "s2t_total": "7822.82", "s2t_n_correct": "4420.89", "t2s_encoder_alpha": "0.1272", "t2s_decoder_alpha": "0.1898", "t2s_enc_dec_attn_loss": "0.00336202", "s2t_ppl": "4.09", "wps": "20994", "ups": "0.87", "wpb": "24082.3", "bsz": "216.4", "num_updates": "700", "lr": "2.848e-06", "gnorm": "2.958", "clip": "0", "loss_scale": "128", "train_wall": "80", "gb_free": "29.9", "wall": "681"}
2024-05-27 19:57:47 | INFO | train_inner | {"epoch": 2, "update": 1.183, "s2t_loss": "4.589", "t2s_loss": "0.58461", "loss": "1.5362", "s2t_nll_loss": "1.886", "ctc_loss": "7.292", "ce_loss": "1.307", "s2t_accuracy": "59.209", "t2s_l1_loss": "0.55742", "t2s_l2_loss": "0.28128", "t2s_bce_loss": "0.02384", "s2t_total": "8560.44", "s2t_n_correct": "5068.55", "t2s_encoder_alpha": "0.12722", "t2s_decoder_alpha": "0.18967", "t2s_enc_dec_attn_loss": "0.00335613", "s2t_ppl": "3.7", "wps": "28936.3", "ups": "1.2", "wpb": "24186.2", "bsz": "212.2", "num_updates": "800", "lr": "3.112e-06", "gnorm": "3.046", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "18.8", "wall": "764"}
2024-05-27 19:59:09 | INFO | train_inner | {"epoch": 2, "update": 1.331, "s2t_loss": "4.366", "t2s_loss": "0.57999", "loss": "1.41893", "s2t_nll_loss": "1.857", "ctc_loss": "6.875", "ce_loss": "1.287", "s2t_accuracy": "59.71", "t2s_l1_loss": "0.5544", "t2s_l2_loss": "0.27758", "t2s_bce_loss": "0.02236", "s2t_total": "8056.86", "s2t_n_correct": "4810.76", "t2s_encoder_alpha": "0.1273", "t2s_decoder_alpha": "0.18958", "t2s_enc_dec_attn_loss": "0.00321937", "s2t_ppl": "3.62", "wps": "29426.7", "ups": "1.21", "wpb": "24229.6", "bsz": "211.7", "num_updates": "900", "lr": "3.376e-06", "gnorm": "2.618", "clip": "0", "loss_scale": "128", "train_wall": "82", "gb_free": "15.1", "wall": "847"}
2024-05-27 20:00:32 | INFO | train_inner | {"epoch": 2, "update": 1.479, "s2t_loss": "4.125", "t2s_loss": "0.57853", "loss": "1.35429", "s2t_nll_loss": "1.818", "ctc_loss": "6.431", "ce_loss": "1.26", "s2t_accuracy": "60.546", "t2s_l1_loss": "0.55243", "t2s_l2_loss": "0.27648", "t2s_bce_loss": "0.02262", "s2t_total": "7786.8", "s2t_n_correct": "4714.63", "t2s_encoder_alpha": "0.12732", "t2s_decoder_alpha": "0.18942", "t2s_enc_dec_attn_loss": "0.0034695", "s2t_ppl": "3.53", "wps": "29123.6", "ups": "1.2", "wpb": "24179.1", "bsz": "218.3", "num_updates": "1000", "lr": "3.64e-06", "gnorm": "2.351", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "22", "wall": "930"}
2024-05-27 20:01:54 | INFO | train_inner | {"epoch": 2, "update": 1.627, "s2t_loss": "3.914", "t2s_loss": "0.57225", "loss": "1.28189", "s2t_nll_loss": "1.784", "ctc_loss": "6.045", "ce_loss": "1.236", "s2t_accuracy": "61.147", "t2s_l1_loss": "0.54809", "t2s_l2_loss": "0.27212", "t2s_bce_loss": "0.02086", "s2t_total": "8054.78", "s2t_n_correct": "4925.27", "t2s_encoder_alpha": "0.12732", "t2s_decoder_alpha": "0.18925", "t2s_enc_dec_attn_loss": "0.00330765", "s2t_ppl": "3.44", "wps": "29459.5", "ups": "1.22", "wpb": "24213", "bsz": "216.5", "num_updates": "1100", "lr": "3.904e-06", "gnorm": "2.059", "clip": "0", "loss_scale": "128", "train_wall": "82", "gb_free": "20.2", "wall": "1012"}
2024-05-27 20:03:18 | INFO | train_inner | {"epoch": 2, "update": 1.775, "s2t_loss": "3.774", "t2s_loss": "0.577", "loss": "1.32124", "s2t_nll_loss": "1.771", "ctc_loss": "5.776", "ce_loss": "1.228", "s2t_accuracy": "61.471", "t2s_l1_loss": "0.54778", "t2s_l2_loss": "0.27287", "t2s_bce_loss": "0.02562", "s2t_total": "8360.2", "s2t_n_correct": "5139.14", "t2s_encoder_alpha": "0.12735", "t2s_decoder_alpha": "0.18911", "t2s_enc_dec_attn_loss": "0.00360271", "s2t_ppl": "3.41", "wps": "28767.9", "ups": "1.19", "wpb": "24108.8", "bsz": "217.1", "num_updates": "1200", "lr": "4.168e-06", "gnorm": "2.053", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "20.4", "wall": "1096"}
2024-05-27 20:04:41 | INFO | train_inner | {"epoch": 2, "update": 1.923, "s2t_loss": "3.61", "t2s_loss": "0.56965", "loss": "1.26358", "s2t_nll_loss": "1.785", "ctc_loss": "5.435", "ce_loss": "1.237", "s2t_accuracy": "61.093", "t2s_l1_loss": "0.54606", "t2s_l2_loss": "0.2708", "t2s_bce_loss": "0.02028", "s2t_total": "8263.7", "s2t_n_correct": "5048.53", "t2s_encoder_alpha": "0.12744", "t2s_decoder_alpha": "0.18906", "t2s_enc_dec_attn_loss": "0.00329714", "s2t_ppl": "3.45", "wps": "28992.2", "ups": "1.2", "wpb": "24163.4", "bsz": "212.1", "num_updates": "1300", "lr": "4.432e-06", "gnorm": "1.66", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "21.9", "wall": "1179"}
2024-05-27 20:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1352 updates
2024-05-27 20:05:24 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint2.pt
2024-05-27 20:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint2.pt
2024-05-27 20:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint2.pt (epoch 2 @ 1352 updates, score None) (writing took 6.133376658894122 seconds)
2024-05-27 20:05:30 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-05-27 20:05:30 | INFO | train | {"epoch": 2, "train_s2t_loss": "4.056", "train_t2s_loss": "0.57691", "train_loss": "1.35432", "train_s2t_nll_loss": "1.821", "train_ctc_loss": "6.29", "train_ce_loss": "1.262", "train_s2t_accuracy": "60.473", "train_t2s_l1_loss": "0.55104", "train_t2s_l2_loss": "0.27518", "train_t2s_bce_loss": "0.02251", "train_s2t_total": "8142.52", "train_s2t_n_correct": "4924.02", "train_t2s_encoder_alpha": "0.12733", "train_t2s_decoder_alpha": "0.18933", "train_t2s_enc_dec_attn_loss": "0.00335406", "train_s2t_ppl": "3.53", "train_wps": "27508.4", "train_ups": "1.14", "train_wpb": "24179.9", "train_bsz": "214.5", "train_num_updates": "1352", "train_lr": "4.56928e-06", "train_gnorm": "2.256", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "557", "train_gb_free": "22.6", "train_wall": "1228"}
2024-05-27 20:05:30 | INFO | fairseq.trainer | loading train data for epoch 3
2024-05-27 20:05:30 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:05:30 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:05:30 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:05:30 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:05:30 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:05:30 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:05:30 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:05:30 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:05:30 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:05:30 | INFO | fairseq.trainer | begin training epoch 3
2024-05-27 20:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 20:06:38 | INFO | train_inner | {"epoch": 3, "update": 2.071, "s2t_loss": "3.483", "t2s_loss": "0.56604", "loss": "1.20173", "s2t_nll_loss": "1.757", "ctc_loss": "5.209", "ce_loss": "1.218", "s2t_accuracy": "61.73", "t2s_l1_loss": "0.54307", "t2s_l2_loss": "0.26786", "t2s_bce_loss": "0.01972", "s2t_total": "8007.73", "s2t_n_correct": "4943.2", "t2s_encoder_alpha": "0.12744", "t2s_decoder_alpha": "0.18891", "t2s_enc_dec_attn_loss": "0.00324598", "s2t_ppl": "3.38", "wps": "20578.4", "ups": "0.85", "wpb": "24112", "bsz": "212.3", "num_updates": "1400", "lr": "4.696e-06", "gnorm": "1.441", "clip": "0", "loss_scale": "128", "train_wall": "82", "gb_free": "19.3", "wall": "1296"}
2024-05-27 20:08:02 | INFO | train_inner | {"epoch": 3, "update": 2.219, "s2t_loss": "3.355", "t2s_loss": "0.56491", "loss": "1.19034", "s2t_nll_loss": "1.738", "ctc_loss": "4.973", "ce_loss": "1.204", "s2t_accuracy": "62.055", "t2s_l1_loss": "0.54004", "t2s_l2_loss": "0.26453", "t2s_bce_loss": "0.02119", "s2t_total": "8137.63", "s2t_n_correct": "5049.81", "t2s_encoder_alpha": "0.12744", "t2s_decoder_alpha": "0.18874", "t2s_enc_dec_attn_loss": "0.00368082", "s2t_ppl": "3.33", "wps": "28919.9", "ups": "1.19", "wpb": "24270.7", "bsz": "230.9", "num_updates": "1500", "lr": "4.96e-06", "gnorm": "1.198", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "20.4", "wall": "1380"}
2024-05-27 20:10:04 | INFO | train_inner | {"epoch": 3, "update": 2.367, "s2t_loss": "3.261", "t2s_loss": "0.56079", "loss": "1.09846", "s2t_nll_loss": "1.717", "ctc_loss": "4.805", "ce_loss": "1.19", "s2t_accuracy": "62.428", "t2s_l1_loss": "0.53815", "t2s_l2_loss": "0.26336", "t2s_bce_loss": "0.01924", "s2t_total": "7380.91", "s2t_n_correct": "4607.78", "t2s_encoder_alpha": "0.12754", "t2s_decoder_alpha": "0.18855", "t2s_enc_dec_attn_loss": "0.00338805", "s2t_ppl": "3.29", "wps": "19883.3", "ups": "0.82", "wpb": "24221.2", "bsz": "221", "num_updates": "1600", "lr": "5.224e-06", "gnorm": "1.005", "clip": "0", "loss_scale": "128", "train_wall": "121", "gb_free": "18.5", "wall": "1502"}
2024-05-27 20:11:28 | INFO | train_inner | {"epoch": 3, "update": 2.515, "s2t_loss": "3.166", "t2s_loss": "0.56435", "loss": "1.12915", "s2t_nll_loss": "1.701", "ctc_loss": "4.632", "ce_loss": "1.179", "s2t_accuracy": "62.805", "t2s_l1_loss": "0.53874", "t2s_l2_loss": "0.26481", "t2s_bce_loss": "0.02229", "s2t_total": "8177.85", "s2t_n_correct": "5136.1", "t2s_encoder_alpha": "0.12756", "t2s_decoder_alpha": "0.18839", "t2s_enc_dec_attn_loss": "0.00330674", "s2t_ppl": "3.25", "wps": "28981.7", "ups": "1.2", "wpb": "24169.6", "bsz": "208.5", "num_updates": "1700", "lr": "5.488e-06", "gnorm": "0.966", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "18.6", "wall": "1586"}
2024-05-27 20:12:52 | INFO | train_inner | {"epoch": 3, "update": 2.663, "s2t_loss": "3.117", "t2s_loss": "0.55821", "loss": "1.15526", "s2t_nll_loss": "1.713", "ctc_loss": "4.521", "ce_loss": "1.188", "s2t_accuracy": "62.633", "t2s_l1_loss": "0.53638", "t2s_l2_loss": "0.26122", "t2s_bce_loss": "0.0187", "s2t_total": "8605.55", "s2t_n_correct": "5389.92", "t2s_encoder_alpha": "0.12756", "t2s_decoder_alpha": "0.1883", "t2s_enc_dec_attn_loss": "0.00313836", "s2t_ppl": "3.28", "wps": "28801.8", "ups": "1.19", "wpb": "24184.8", "bsz": "205.8", "num_updates": "1800", "lr": "5.752e-06", "gnorm": "0.858", "clip": "0", "loss_scale": "128", "train_wall": "83", "gb_free": "30.9", "wall": "1670"}
2024-05-27 20:14:14 | INFO | train_inner | {"epoch": 3, "update": 2.811, "s2t_loss": "3.081", "t2s_loss": "0.5545", "loss": "1.10189", "s2t_nll_loss": "1.729", "ctc_loss": "4.433", "ce_loss": "1.198", "s2t_accuracy": "62.238", "t2s_l1_loss": "0.53387", "t2s_l2_loss": "0.25936", "t2s_bce_loss": "0.01755", "s2t_total": "7932.75", "s2t_n_correct": "4937.16", "t2s_encoder_alpha": "0.12756", "t2s_decoder_alpha": "0.18819", "t2s_enc_dec_attn_loss": "0.00307559", "s2t_ppl": "3.31", "wps": "29311.2", "ups": "1.21", "wpb": "24233.1", "bsz": "208.7", "num_updates": "1900", "lr": "6.016e-06", "gnorm": "0.721", "clip": "0", "loss_scale": "128", "train_wall": "82", "gb_free": "20.5", "wall": "1752"}
2024-05-27 20:16:18 | INFO | train_inner | {"epoch": 3, "update": 2.959, "s2t_loss": "3.023", "t2s_loss": "0.55493", "loss": "1.08836", "s2t_nll_loss": "1.705", "ctc_loss": "4.342", "ce_loss": "1.182", "s2t_accuracy": "62.738", "t2s_l1_loss": "0.53362", "t2s_l2_loss": "0.25913", "t2s_bce_loss": "0.01808", "s2t_total": "8140.65", "s2t_n_correct": "5107.29", "t2s_encoder_alpha": "0.12763", "t2s_decoder_alpha": "0.18803", "t2s_enc_dec_attn_loss": "0.00321398", "s2t_ppl": "3.26", "wps": "19610.4", "ups": "0.81", "wpb": "24190", "bsz": "213.9", "num_updates": "2000", "lr": "6.28e-06", "gnorm": "0.683", "clip": "0", "loss_scale": "128", "train_wall": "123", "gb_free": "22", "wall": "1876"}
2024-05-27 20:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2028 updates
2024-05-27 20:16:40 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint3.pt
2024-05-27 20:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint3.pt
2024-05-27 20:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint3.pt (epoch 3 @ 2028 updates, score None) (writing took 6.300618786830455 seconds)
2024-05-27 20:16:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-05-27 20:16:47 | INFO | train | {"epoch": 3, "train_s2t_loss": "3.181", "train_t2s_loss": "0.55999", "train_loss": "1.13161", "train_s2t_nll_loss": "1.719", "train_ctc_loss": "4.642", "train_ce_loss": "1.192", "train_s2t_accuracy": "62.452", "train_t2s_l1_loss": "0.53718", "train_t2s_l2_loss": "0.26251", "train_t2s_bce_loss": "0.01951", "train_s2t_total": "8052.14", "train_s2t_n_correct": "5028.71", "train_t2s_encoder_alpha": "0.12755", "train_t2s_decoder_alpha": "0.18838", "train_t2s_enc_dec_attn_loss": "0.00329704", "train_s2t_ppl": "3.29", "train_wps": "24150.4", "train_ups": "1", "train_wpb": "24180.6", "train_bsz": "214.5", "train_num_updates": "2028", "train_lr": "6.35392e-06", "train_gnorm": "0.933", "train_clip": "0", "train_loss_scale": "128", "train_train_wall": "639", "train_gb_free": "20.3", "train_wall": "1905"}
2024-05-27 20:16:47 | INFO | fairseq.trainer | loading train data for epoch 4
2024-05-27 20:16:47 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:16:47 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:16:47 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:16:47 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:16:47 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:16:47 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:16:47 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:16:47 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:16:47 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:16:47 | INFO | fairseq.trainer | begin training epoch 4
2024-05-27 20:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 20:18:14 | INFO | train_inner | {"epoch": 4, "update": 3.107, "s2t_loss": "2.988", "t2s_loss": "0.5571", "loss": "1.0569", "s2t_nll_loss": "1.701", "ctc_loss": "4.275", "ce_loss": "1.179", "s2t_accuracy": "62.854", "t2s_l1_loss": "0.53291", "t2s_l2_loss": "0.25924", "t2s_bce_loss": "0.02087", "s2t_total": "7577.45", "s2t_n_correct": "4762.72", "t2s_encoder_alpha": "0.12769", "t2s_decoder_alpha": "0.18787", "t2s_enc_dec_attn_loss": "0.00332348", "s2t_ppl": "3.25", "wps": "20591.3", "ups": "0.86", "wpb": "24043.7", "bsz": "214.9", "num_updates": "2100", "lr": "6.544e-06", "gnorm": "0.718", "clip": "0", "loss_scale": "256", "train_wall": "81", "gb_free": "19.2", "wall": "1992"}
2024-05-27 20:19:38 | INFO | train_inner | {"epoch": 4, "update": 3.254, "s2t_loss": "2.96", "t2s_loss": "0.55074", "loss": "1.0685", "s2t_nll_loss": "1.673", "ctc_loss": "4.246", "ce_loss": "1.16", "s2t_accuracy": "63.393", "t2s_l1_loss": "0.53028", "t2s_l2_loss": "0.25662", "t2s_bce_loss": "0.01726", "s2t_total": "7880.31", "s2t_n_correct": "4995.58", "t2s_encoder_alpha": "0.12769", "t2s_decoder_alpha": "0.18781", "t2s_enc_dec_attn_loss": "0.00317986", "s2t_ppl": "3.19", "wps": "29068.9", "ups": "1.2", "wpb": "24197.7", "bsz": "210.3", "num_updates": "2200", "lr": "6.808e-06", "gnorm": "0.625", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "23", "wall": "2076"}
2024-05-27 20:21:01 | INFO | train_inner | {"epoch": 4, "update": 3.402, "s2t_loss": "2.948", "t2s_loss": "0.5552", "loss": "1.06871", "s2t_nll_loss": "1.685", "ctc_loss": "4.212", "ce_loss": "1.168", "s2t_accuracy": "63.178", "t2s_l1_loss": "0.53277", "t2s_l2_loss": "0.25865", "t2s_bce_loss": "0.01897", "s2t_total": "8117.36", "s2t_n_correct": "5128.36", "t2s_encoder_alpha": "0.12769", "t2s_decoder_alpha": "0.18765", "t2s_enc_dec_attn_loss": "0.00345508", "s2t_ppl": "3.21", "wps": "28986.5", "ups": "1.2", "wpb": "24153.9", "bsz": "226.2", "num_updates": "2300", "lr": "7.072e-06", "gnorm": "0.637", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "22.7", "wall": "2159"}
2024-05-27 20:22:23 | INFO | train_inner | {"epoch": 4, "update": 3.55, "s2t_loss": "2.949", "t2s_loss": "0.54879", "loss": "1.05522", "s2t_nll_loss": "1.707", "ctc_loss": "4.19", "ce_loss": "1.183", "s2t_accuracy": "62.747", "t2s_l1_loss": "0.52717", "t2s_l2_loss": "0.25319", "t2s_bce_loss": "0.01821", "s2t_total": "7851.89", "s2t_n_correct": "4926.84", "t2s_encoder_alpha": "0.12775", "t2s_decoder_alpha": "0.1875", "t2s_enc_dec_attn_loss": "0.00339779", "s2t_ppl": "3.27", "wps": "29492.2", "ups": "1.22", "wpb": "24256", "bsz": "225.8", "num_updates": "2400", "lr": "7.336e-06", "gnorm": "0.62", "clip": "0", "loss_scale": "256", "train_wall": "82", "gb_free": "19.8", "wall": "2241"}
2024-05-27 20:23:49 | INFO | train_inner | {"epoch": 4, "update": 3.698, "s2t_loss": "2.916", "t2s_loss": "0.54635", "loss": "1.09389", "s2t_nll_loss": "1.673", "ctc_loss": "4.16", "ce_loss": "1.159", "s2t_accuracy": "63.303", "t2s_l1_loss": "0.52678", "t2s_l2_loss": "0.25251", "t2s_bce_loss": "0.01658", "s2t_total": "8707.78", "s2t_n_correct": "5512.27", "t2s_encoder_alpha": "0.12781", "t2s_decoder_alpha": "0.18733", "t2s_enc_dec_attn_loss": "0.00298142", "s2t_ppl": "3.19", "wps": "28327", "ups": "1.17", "wpb": "24233.2", "bsz": "201.3", "num_updates": "2500", "lr": "7.6e-06", "gnorm": "0.634", "clip": "0", "loss_scale": "256", "train_wall": "85", "gb_free": "21.3", "wall": "2327"}
2024-05-27 20:25:13 | INFO | train_inner | {"epoch": 4, "update": 3.846, "s2t_loss": "2.913", "t2s_loss": "0.54695", "loss": "1.06591", "s2t_nll_loss": "1.677", "ctc_loss": "4.149", "ce_loss": "1.162", "s2t_accuracy": "63.263", "t2s_l1_loss": "0.52699", "t2s_l2_loss": "0.25314", "t2s_bce_loss": "0.01679", "s2t_total": "8279.35", "s2t_n_correct": "5237.8", "t2s_encoder_alpha": "0.12781", "t2s_decoder_alpha": "0.18719", "t2s_enc_dec_attn_loss": "0.00318039", "s2t_ppl": "3.2", "wps": "28485.6", "ups": "1.18", "wpb": "24134.8", "bsz": "214.2", "num_updates": "2600", "lr": "7.864e-06", "gnorm": "0.574", "clip": "0", "loss_scale": "256", "train_wall": "84", "gb_free": "18.9", "wall": "2411"}
2024-05-27 20:26:37 | INFO | train_inner | {"epoch": 4, "update": 3.994, "s2t_loss": "2.911", "t2s_loss": "0.54571", "loss": "1.05719", "s2t_nll_loss": "1.681", "ctc_loss": "4.14", "ce_loss": "1.166", "s2t_accuracy": "63.227", "t2s_l1_loss": "0.5257", "t2s_l2_loss": "0.25207", "t2s_bce_loss": "0.01701", "s2t_total": "8125.03", "s2t_n_correct": "5137.21", "t2s_encoder_alpha": "0.12788", "t2s_decoder_alpha": "0.18697", "t2s_enc_dec_attn_loss": "0.00299957", "s2t_ppl": "3.21", "wps": "29220.7", "ups": "1.2", "wpb": "24284.2", "bsz": "209.6", "num_updates": "2700", "lr": "8.128e-06", "gnorm": "0.681", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "22", "wall": "2495"}
2024-05-27 20:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2704 updates
2024-05-27 20:26:40 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint4.pt
2024-05-27 20:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint4.pt
2024-05-27 20:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint4.pt (epoch 4 @ 2704 updates, score None) (writing took 6.19412601692602 seconds)
2024-05-27 20:26:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-05-27 20:26:46 | INFO | train | {"epoch": 4, "train_s2t_loss": "2.938", "train_t2s_loss": "0.54989", "train_loss": "1.06666", "train_s2t_nll_loss": "1.685", "train_ctc_loss": "4.191", "train_ce_loss": "1.168", "train_s2t_accuracy": "63.15", "train_t2s_l1_loss": "0.52871", "train_t2s_l2_loss": "0.25486", "train_t2s_bce_loss": "0.01796", "train_s2t_total": "8092.95", "train_s2t_n_correct": "5110.73", "train_t2s_encoder_alpha": "0.12776", "train_t2s_decoder_alpha": "0.18746", "train_t2s_enc_dec_attn_loss": "0.00322048", "train_s2t_ppl": "3.21", "train_wps": "27286.5", "train_ups": "1.13", "train_wpb": "24180.2", "train_bsz": "214.5", "train_num_updates": "2704", "train_lr": "8.13856e-06", "train_gnorm": "0.642", "train_clip": "0", "train_loss_scale": "256", "train_train_wall": "561", "train_gb_free": "30.3", "train_wall": "2504"}
2024-05-27 20:26:46 | INFO | fairseq.trainer | loading train data for epoch 5
2024-05-27 20:26:46 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:26:46 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:26:46 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:26:46 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:26:46 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:26:46 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:26:46 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:26:46 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:26:46 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:26:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:26:46 | INFO | fairseq.trainer | begin training epoch 5
2024-05-27 20:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 20:28:38 | INFO | train_inner | {"epoch": 5, "update": 4.142, "s2t_loss": "2.906", "t2s_loss": "0.54681", "loss": "1.08099", "s2t_nll_loss": "1.68", "ctc_loss": "4.132", "ce_loss": "1.164", "s2t_accuracy": "63.301", "t2s_l1_loss": "0.52631", "t2s_l2_loss": "0.25305", "t2s_bce_loss": "0.01734", "s2t_total": "8537.7", "s2t_n_correct": "5404.49", "t2s_encoder_alpha": "0.12793", "t2s_decoder_alpha": "0.18681", "t2s_enc_dec_attn_loss": "0.00316528", "s2t_ppl": "3.2", "wps": "19738.7", "ups": "0.82", "wpb": "23994", "bsz": "216", "num_updates": "2800", "lr": "8.392e-06", "gnorm": "0.599", "clip": "0", "loss_scale": "256", "train_wall": "84", "gb_free": "21.2", "wall": "2616"}
2024-05-27 20:30:01 | INFO | train_inner | {"epoch": 5, "update": 4.29, "s2t_loss": "2.895", "t2s_loss": "0.54303", "loss": "1.04078", "s2t_nll_loss": "1.667", "ctc_loss": "4.123", "ce_loss": "1.156", "s2t_accuracy": "63.491", "t2s_l1_loss": "0.52388", "t2s_l2_loss": "0.25051", "t2s_bce_loss": "0.01618", "s2t_total": "7992.03", "s2t_n_correct": "5074.22", "t2s_encoder_alpha": "0.12793", "t2s_decoder_alpha": "0.18672", "t2s_enc_dec_attn_loss": "0.00297365", "s2t_ppl": "3.18", "wps": "29102", "ups": "1.2", "wpb": "24233.2", "bsz": "211.2", "num_updates": "2900", "lr": "8.656e-06", "gnorm": "0.592", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "18.7", "wall": "2699"}
2024-05-27 20:31:23 | INFO | train_inner | {"epoch": 5, "update": 4.438, "s2t_loss": "2.886", "t2s_loss": "0.54901", "loss": "1.01702", "s2t_nll_loss": "1.648", "ctc_loss": "4.123", "ce_loss": "1.142", "s2t_accuracy": "63.804", "t2s_l1_loss": "0.52355", "t2s_l2_loss": "0.25166", "t2s_bce_loss": "0.02207", "s2t_total": "7599.03", "s2t_n_correct": "4848.46", "t2s_encoder_alpha": "0.12793", "t2s_decoder_alpha": "0.18662", "t2s_enc_dec_attn_loss": "0.00339312", "s2t_ppl": "3.13", "wps": "29796.6", "ups": "1.23", "wpb": "24218.3", "bsz": "223.5", "num_updates": "3000", "lr": "8.92e-06", "gnorm": "0.714", "clip": "0", "loss_scale": "256", "train_wall": "81", "gb_free": "19.8", "wall": "2781"}
2024-05-27 20:32:49 | INFO | train_inner | {"epoch": 5, "update": 4.586, "s2t_loss": "2.883", "t2s_loss": "0.54095", "loss": "1.09313", "s2t_nll_loss": "1.652", "ctc_loss": "4.115", "ce_loss": "1.145", "s2t_accuracy": "63.793", "t2s_l1_loss": "0.5226", "t2s_l2_loss": "0.24958", "t2s_bce_loss": "0.01556", "s2t_total": "8776.23", "s2t_n_correct": "5598.65", "t2s_encoder_alpha": "0.128", "t2s_decoder_alpha": "0.18652", "t2s_enc_dec_attn_loss": "0.00278899", "s2t_ppl": "3.14", "wps": "27866.8", "ups": "1.15", "wpb": "24128", "bsz": "201.6", "num_updates": "3100", "lr": "9.184e-06", "gnorm": "0.55", "clip": "0", "loss_scale": "256", "train_wall": "86", "gb_free": "20", "wall": "2867"}
2024-05-27 20:34:13 | INFO | train_inner | {"epoch": 5, "update": 4.734, "s2t_loss": "2.877", "t2s_loss": "0.54138", "loss": "1.04082", "s2t_nll_loss": "1.638", "ctc_loss": "4.117", "ce_loss": "1.136", "s2t_accuracy": "64.082", "t2s_l1_loss": "0.5211", "t2s_l2_loss": "0.2484", "t2s_bce_loss": "0.01708", "s2t_total": "8122.84", "s2t_n_correct": "5205.3", "t2s_encoder_alpha": "0.12805", "t2s_decoder_alpha": "0.18639", "t2s_enc_dec_attn_loss": "0.00320677", "s2t_ppl": "3.11", "wps": "29053.7", "ups": "1.2", "wpb": "24186.9", "bsz": "225.5", "num_updates": "3200", "lr": "9.448e-06", "gnorm": "0.579", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "21.4", "wall": "2951"}
2024-05-27 20:35:33 | INFO | train_inner | {"epoch": 5, "update": 4.882, "s2t_loss": "2.885", "t2s_loss": "0.54065", "loss": "1.03143", "s2t_nll_loss": "1.655", "ctc_loss": "4.115", "ce_loss": "1.147", "s2t_accuracy": "63.809", "t2s_l1_loss": "0.52169", "t2s_l2_loss": "0.24872", "t2s_bce_loss": "0.01619", "s2t_total": "7782.04", "s2t_n_correct": "4965.61", "t2s_encoder_alpha": "0.12805", "t2s_decoder_alpha": "0.18625", "t2s_enc_dec_attn_loss": "0.00277513", "s2t_ppl": "3.15", "wps": "30019.8", "ups": "1.24", "wpb": "24235.9", "bsz": "213.3", "num_updates": "3300", "lr": "9.712e-06", "gnorm": "0.612", "clip": "0", "loss_scale": "256", "train_wall": "80", "gb_free": "21.3", "wall": "3031"}
2024-05-27 20:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3380 updates
2024-05-27 20:36:39 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint5.pt
2024-05-27 20:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint5.pt
2024-05-27 20:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint5.pt (epoch 5 @ 3380 updates, score None) (writing took 5.9188450002111495 seconds)
2024-05-27 20:36:45 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-05-27 20:36:45 | INFO | train | {"epoch": 5, "train_s2t_loss": "2.889", "train_t2s_loss": "0.54372", "train_loss": "1.0506", "train_s2t_nll_loss": "1.658", "train_ctc_loss": "4.12", "train_ce_loss": "1.149", "train_s2t_accuracy": "63.688", "train_t2s_l1_loss": "0.52339", "train_t2s_l2_loss": "0.25052", "train_t2s_bce_loss": "0.01732", "train_s2t_total": "8150.92", "train_s2t_n_correct": "5191.13", "train_t2s_encoder_alpha": "0.128", "train_t2s_decoder_alpha": "0.1865", "train_t2s_enc_dec_attn_loss": "0.00300801", "train_s2t_ppl": "3.16", "train_wps": "27294.9", "train_ups": "1.13", "train_wpb": "24180.5", "train_bsz": "214.5", "train_num_updates": "3380", "train_lr": "9.9232e-06", "train_gnorm": "0.622", "train_clip": "0", "train_loss_scale": "256", "train_train_wall": "559", "train_gb_free": "30.4", "train_wall": "3103"}
2024-05-27 20:36:45 | INFO | fairseq.trainer | loading train data for epoch 6
2024-05-27 20:36:45 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:36:45 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:36:45 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:36:45 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:36:45 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:36:45 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:36:45 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:36:45 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:36:45 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:36:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:36:45 | INFO | fairseq.trainer | begin training epoch 6
2024-05-27 20:36:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 20:37:31 | INFO | train_inner | {"epoch": 6, "update": 5.03, "s2t_loss": "2.89", "t2s_loss": "0.54272", "loss": "1.05775", "s2t_nll_loss": "1.665", "ctc_loss": "4.114", "ce_loss": "1.154", "s2t_accuracy": "63.49", "t2s_l1_loss": "0.52415", "t2s_l2_loss": "0.25129", "t2s_bce_loss": "0.01598", "s2t_total": "8409.2", "s2t_n_correct": "5339.02", "t2s_encoder_alpha": "0.12811", "t2s_decoder_alpha": "0.18611", "t2s_enc_dec_attn_loss": "0.00257764", "s2t_ppl": "3.17", "wps": "20387.4", "ups": "0.85", "wpb": "24074.6", "bsz": "204", "num_updates": "3400", "lr": "9.976e-06", "gnorm": "0.738", "clip": "0", "loss_scale": "256", "train_wall": "81", "gb_free": "21.8", "wall": "3149"}
2024-05-27 20:38:54 | INFO | train_inner | {"epoch": 6, "update": 5.178, "s2t_loss": "2.881", "t2s_loss": "0.54156", "loss": "1.03828", "s2t_nll_loss": "1.651", "ctc_loss": "4.111", "ce_loss": "1.144", "s2t_accuracy": "63.869", "t2s_l1_loss": "0.51914", "t2s_l2_loss": "0.24739", "t2s_bce_loss": "0.01942", "s2t_total": "7788.02", "s2t_n_correct": "4974.15", "t2s_encoder_alpha": "0.12817", "t2s_decoder_alpha": "0.18596", "t2s_enc_dec_attn_loss": "0.00301558", "s2t_ppl": "3.14", "wps": "29501.7", "ups": "1.22", "wpb": "24245.1", "bsz": "222.7", "num_updates": "3500", "lr": "1.024e-05", "gnorm": "0.659", "clip": "0", "loss_scale": "256", "train_wall": "82", "gb_free": "20.3", "wall": "3232"}
2024-05-27 20:40:17 | INFO | train_inner | {"epoch": 6, "update": 5.325, "s2t_loss": "2.866", "t2s_loss": "0.5357", "loss": "1.04911", "s2t_nll_loss": "1.621", "ctc_loss": "4.112", "ce_loss": "1.123", "s2t_accuracy": "64.439", "t2s_l1_loss": "0.51643", "t2s_l2_loss": "0.2438", "t2s_bce_loss": "0.0165", "s2t_total": "8158.39", "s2t_n_correct": "5257.14", "t2s_encoder_alpha": "0.12817", "t2s_decoder_alpha": "0.18585", "t2s_enc_dec_attn_loss": "0.0027694", "s2t_ppl": "3.08", "wps": "28976.8", "ups": "1.2", "wpb": "24182.8", "bsz": "213.5", "num_updates": "3600", "lr": "1.0504e-05", "gnorm": "0.608", "clip": "0", "loss_scale": "256", "train_wall": "83", "gb_free": "21.3", "wall": "3315"}
2024-05-27 20:41:42 | INFO | train_inner | {"epoch": 6, "update": 5.473, "s2t_loss": "2.87", "t2s_loss": "0.53557", "loss": "1.0627", "s2t_nll_loss": "1.631", "ctc_loss": "4.11", "ce_loss": "1.13", "s2t_accuracy": "64.179", "t2s_l1_loss": "0.51796", "t2s_l2_loss": "0.24545", "t2s_bce_loss": "0.01515", "s2t_total": "8375.09", "s2t_n_correct": "5375.08", "t2s_encoder_alpha": "0.12818", "t2s_decoder_alpha": "0.18574", "t2s_enc_dec_attn_loss": "0.00245241", "s2t_ppl": "3.1", "wps": "28546.6", "ups": "1.18", "wpb": "24178", "bsz": "202.3", "num_updates": "3700", "lr": "1.0768e-05", "gnorm": "0.583", "clip": "0", "loss_scale": "256", "train_wall": "84", "gb_free": "19.9", "wall": "3400"}
2024-05-27 20:43:03 | INFO | train_inner | {"epoch": 6, "update": 5.621, "s2t_loss": "2.869", "t2s_loss": "0.535", "loss": "1.01674", "s2t_nll_loss": "1.633", "ctc_loss": "4.106", "ce_loss": "1.132", "s2t_accuracy": "64.125", "t2s_l1_loss": "0.5161", "t2s_l2_loss": "0.24413", "t2s_bce_loss": "0.01617", "s2t_total": "7770.58", "s2t_n_correct": "4982.9", "t2s_encoder_alpha": "0.12828", "t2s_decoder_alpha": "0.18559", "t2s_enc_dec_attn_loss": "0.00272595", "s2t_ppl": "3.1", "wps": "29980", "ups": "1.24", "wpb": "24252.4", "bsz": "225", "num_updates": "3800", "lr": "1.1032e-05", "gnorm": "0.612", "clip": "0", "loss_scale": "256", "train_wall": "80", "gb_free": "29.9", "wall": "3481"}
2024-05-27 20:44:25 | INFO | train_inner | {"epoch": 6, "update": 5.769, "s2t_loss": "2.857", "t2s_loss": "0.53268", "loss": "1.01397", "s2t_nll_loss": "1.61", "ctc_loss": "4.104", "ce_loss": "1.116", "s2t_accuracy": "64.596", "t2s_l1_loss": "0.51356", "t2s_l2_loss": "0.2418", "t2s_bce_loss": "0.01668", "s2t_total": "7982.34", "s2t_n_correct": "5156.31", "t2s_encoder_alpha": "0.1283", "t2s_decoder_alpha": "0.18545", "t2s_enc_dec_attn_loss": "0.00243331", "s2t_ppl": "3.05", "wps": "29531.8", "ups": "1.22", "wpb": "24192.2", "bsz": "213.4", "num_updates": "3900", "lr": "1.1296e-05", "gnorm": "0.621", "clip": "0", "loss_scale": "256", "train_wall": "81", "gb_free": "18.3", "wall": "3563"}
2024-05-27 20:45:47 | INFO | train_inner | {"epoch": 6, "update": 5.917, "s2t_loss": "2.878", "t2s_loss": "0.53563", "loss": "1.0483", "s2t_nll_loss": "1.651", "ctc_loss": "4.105", "ce_loss": "1.144", "s2t_accuracy": "63.704", "t2s_l1_loss": "0.51681", "t2s_l2_loss": "0.24559", "t2s_bce_loss": "0.01638", "s2t_total": "8233.72", "s2t_n_correct": "5245.22", "t2s_encoder_alpha": "0.12836", "t2s_decoder_alpha": "0.18532", "t2s_enc_dec_attn_loss": "0.00244155", "s2t_ppl": "3.14", "wps": "29259.4", "ups": "1.21", "wpb": "24187.8", "bsz": "218", "num_updates": "4000", "lr": "1.156e-05", "gnorm": "0.637", "clip": "0", "loss_scale": "256", "train_wall": "82", "gb_free": "30.6", "wall": "3645"}
2024-05-27 20:46:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4056 updates
2024-05-27 20:46:33 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint6.pt
2024-05-27 20:46:37 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint6.pt
2024-05-27 20:46:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint6.pt (epoch 6 @ 4056 updates, score None) (writing took 6.394469053018838 seconds)
2024-05-27 20:46:40 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-05-27 20:46:40 | INFO | train | {"epoch": 6, "train_s2t_loss": "2.872", "train_t2s_loss": "0.53576", "train_loss": "1.0413", "train_s2t_nll_loss": "1.635", "train_ctc_loss": "4.108", "train_ce_loss": "1.134", "train_s2t_accuracy": "64.094", "train_t2s_l1_loss": "0.51661", "train_t2s_l2_loss": "0.24463", "train_t2s_bce_loss": "0.01655", "train_s2t_total": "8113.31", "train_s2t_n_correct": "5200.12", "train_t2s_encoder_alpha": "0.12825", "train_t2s_decoder_alpha": "0.18563", "train_t2s_enc_dec_attn_loss": "0.00260015", "train_s2t_ppl": "3.11", "train_wps": "27470.6", "train_ups": "1.14", "train_wpb": "24180.3", "train_bsz": "214.5", "train_num_updates": "4056", "train_lr": "1.17078e-05", "train_gnorm": "0.621", "train_clip": "0", "train_loss_scale": "256", "train_train_wall": "555", "train_gb_free": "20", "train_wall": "3698"}
2024-05-27 20:46:40 | INFO | fairseq.trainer | loading train data for epoch 7
2024-05-27 20:46:40 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:46:40 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:46:40 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:46:40 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:46:40 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:46:40 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:46:40 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:46:40 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:46:40 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:46:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:46:40 | INFO | fairseq.trainer | begin training epoch 7
2024-05-27 20:46:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 20:47:47 | INFO | train_inner | {"epoch": 7, "update": 6.065, "s2t_loss": "2.873", "t2s_loss": "0.53088", "loss": "1.04968", "s2t_nll_loss": "1.642", "ctc_loss": "4.104", "ce_loss": "1.138", "s2t_accuracy": "63.955", "t2s_l1_loss": "0.51375", "t2s_l2_loss": "0.24197", "t2s_bce_loss": "0.01486", "s2t_total": "8612.32", "s2t_n_correct": "5508.01", "t2s_encoder_alpha": "0.12842", "t2s_decoder_alpha": "0.18517", "t2s_enc_dec_attn_loss": "0.00226145", "s2t_ppl": "3.12", "wps": "20144.1", "ups": "0.84", "wpb": "24041.4", "bsz": "208.6", "num_updates": "4100", "lr": "1.1824e-05", "gnorm": "0.59", "clip": "0", "loss_scale": "512", "train_wall": "82", "gb_free": "21.2", "wall": "3765"}
2024-05-27 20:49:09 | INFO | train_inner | {"epoch": 7, "update": 6.213, "s2t_loss": "2.868", "t2s_loss": "0.52865", "loss": "1.02655", "s2t_nll_loss": "1.637", "ctc_loss": "4.098", "ce_loss": "1.135", "s2t_accuracy": "64.054", "t2s_l1_loss": "0.51192", "t2s_l2_loss": "0.2407", "t2s_bce_loss": "0.01463", "s2t_total": "7813.68", "s2t_n_correct": "5004.94", "t2s_encoder_alpha": "0.12842", "t2s_decoder_alpha": "0.18505", "t2s_enc_dec_attn_loss": "0.00209945", "s2t_ppl": "3.11", "wps": "29561.7", "ups": "1.22", "wpb": "24280.6", "bsz": "208.8", "num_updates": "4200", "lr": "1.2088e-05", "gnorm": "0.602", "clip": "0", "loss_scale": "512", "train_wall": "82", "gb_free": "20.3", "wall": "3847"}
2024-05-27 20:51:54 | INFO | train_inner | {"epoch": 7, "update": 6.361, "s2t_loss": "2.866", "t2s_loss": "0.52995", "loss": "1.05613", "s2t_nll_loss": "1.635", "ctc_loss": "4.097", "ce_loss": "1.133", "s2t_accuracy": "64.021", "t2s_l1_loss": "0.51176", "t2s_l2_loss": "0.24023", "t2s_bce_loss": "0.01585", "s2t_total": "8406.12", "s2t_n_correct": "5381.72", "t2s_encoder_alpha": "0.12842", "t2s_decoder_alpha": "0.18494", "t2s_enc_dec_attn_loss": "0.00233365", "s2t_ppl": "3.11", "wps": "14672.7", "ups": "0.61", "wpb": "24185.6", "bsz": "220.3", "num_updates": "4300", "lr": "1.2352e-05", "gnorm": "0.598", "clip": "0", "loss_scale": "512", "train_wall": "164", "gb_free": "19.1", "wall": "4012"}
2024-05-27 20:53:16 | INFO | train_inner | {"epoch": 7, "update": 6.509, "s2t_loss": "2.862", "t2s_loss": "0.5304", "loss": "1.04627", "s2t_nll_loss": "1.629", "ctc_loss": "4.094", "ce_loss": "1.129", "s2t_accuracy": "64.128", "t2s_l1_loss": "0.51378", "t2s_l2_loss": "0.24283", "t2s_bce_loss": "0.01469", "s2t_total": "8090.6", "s2t_n_correct": "5188.33", "t2s_encoder_alpha": "0.12851", "t2s_decoder_alpha": "0.18484", "t2s_enc_dec_attn_loss": "0.00193215", "s2t_ppl": "3.09", "wps": "29188", "ups": "1.21", "wpb": "24089", "bsz": "199.6", "num_updates": "4400", "lr": "1.2616e-05", "gnorm": "0.688", "clip": "0", "loss_scale": "512", "train_wall": "82", "gb_free": "21.3", "wall": "4094"}
2024-05-27 20:54:37 | INFO | train_inner | {"epoch": 7, "update": 6.657, "s2t_loss": "2.871", "t2s_loss": "0.5289", "loss": "1.00558", "s2t_nll_loss": "1.644", "ctc_loss": "4.097", "ce_loss": "1.14", "s2t_accuracy": "63.908", "t2s_l1_loss": "0.51105", "t2s_l2_loss": "0.23991", "t2s_bce_loss": "0.01576", "s2t_total": "7943.76", "s2t_n_correct": "5076.69", "t2s_encoder_alpha": "0.12854", "t2s_decoder_alpha": "0.18473", "t2s_enc_dec_attn_loss": "0.00209457", "s2t_ppl": "3.13", "wps": "29817.9", "ups": "1.23", "wpb": "24236.1", "bsz": "217.2", "num_updates": "4500", "lr": "1.288e-05", "gnorm": "0.637", "clip": "0", "loss_scale": "512", "train_wall": "81", "gb_free": "20.9", "wall": "4175"}
2024-05-27 20:55:59 | INFO | train_inner | {"epoch": 7, "update": 6.805, "s2t_loss": "2.852", "t2s_loss": "0.52637", "loss": "1.05222", "s2t_nll_loss": "1.611", "ctc_loss": "4.093", "ce_loss": "1.117", "s2t_accuracy": "64.516", "t2s_l1_loss": "0.5101", "t2s_l2_loss": "0.23876", "t2s_bce_loss": "0.01444", "s2t_total": "8487.07", "s2t_n_correct": "5475.52", "t2s_encoder_alpha": "0.12854", "t2s_decoder_alpha": "0.18468", "t2s_enc_dec_attn_loss": "0.00184385", "s2t_ppl": "3.06", "wps": "29671.4", "ups": "1.23", "wpb": "24162", "bsz": "201.8", "num_updates": "4600", "lr": "1.3144e-05", "gnorm": "0.605", "clip": "0", "loss_scale": "512", "train_wall": "81", "gb_free": "20.1", "wall": "4257"}
2024-05-27 20:57:23 | INFO | train_inner | {"epoch": 7, "update": 6.953, "s2t_loss": "2.854", "t2s_loss": "0.53686", "loss": "1.05221", "s2t_nll_loss": "1.615", "ctc_loss": "4.093", "ce_loss": "1.12", "s2t_accuracy": "64.474", "t2s_l1_loss": "0.51203", "t2s_l2_loss": "0.2413", "t2s_bce_loss": "0.02222", "s2t_total": "8197.56", "s2t_n_correct": "5285.28", "t2s_encoder_alpha": "0.12852", "t2s_decoder_alpha": "0.18463", "t2s_enc_dec_attn_loss": "0.00261974", "s2t_ppl": "3.06", "wps": "28799.3", "ups": "1.19", "wpb": "24213.7", "bsz": "240.6", "num_updates": "4700", "lr": "1.3408e-05", "gnorm": "0.753", "clip": "0", "loss_scale": "512", "train_wall": "84", "gb_free": "22.1", "wall": "4341"}
2024-05-27 20:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 4732 updates
2024-05-27 20:57:48 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint7.pt
2024-05-27 20:57:52 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint7.pt
2024-05-27 20:57:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint7.pt (epoch 7 @ 4732 updates, score None) (writing took 8.414995413273573 seconds)
2024-05-27 20:57:56 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-05-27 20:57:56 | INFO | train | {"epoch": 7, "train_s2t_loss": "2.861", "train_t2s_loss": "0.52992", "train_loss": "1.03495", "train_s2t_nll_loss": "1.627", "train_ctc_loss": "4.096", "train_ce_loss": "1.128", "train_s2t_accuracy": "64.225", "train_t2s_l1_loss": "0.51173", "train_t2s_l2_loss": "0.24053", "train_t2s_bce_loss": "0.01605", "train_s2t_total": "8125.92", "train_s2t_n_correct": "5218.88", "train_t2s_encoder_alpha": "0.12848", "train_t2s_decoder_alpha": "0.18481", "train_t2s_enc_dec_attn_loss": "0.00214421", "train_s2t_ppl": "3.09", "train_wps": "24151.3", "train_ups": "1", "train_wpb": "24180.5", "train_bsz": "214.5", "train_num_updates": "4732", "train_lr": "1.34925e-05", "train_gnorm": "0.64", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "634", "train_gb_free": "29.5", "train_wall": "4374"}
2024-05-27 20:57:56 | INFO | fairseq.trainer | loading train data for epoch 8
2024-05-27 20:57:56 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 20:57:56 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 20:57:57 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 20:57:57 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 20:57:57 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 20:57:57 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 20:57:57 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 20:57:57 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 20:57:57 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 20:57:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 20:57:57 | INFO | fairseq.trainer | begin training epoch 8
2024-05-27 20:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 21:01:07 | INFO | train_inner | {"epoch": 8, "update": 7.101, "s2t_loss": "2.851", "t2s_loss": "0.52538", "loss": "1.0041", "s2t_nll_loss": "1.607", "ctc_loss": "4.095", "ce_loss": "1.114", "s2t_accuracy": "64.641", "t2s_l1_loss": "0.50843", "t2s_l2_loss": "0.2375", "t2s_bce_loss": "0.015", "s2t_total": "7879.51", "s2t_n_correct": "5093.41", "t2s_encoder_alpha": "0.12843", "t2s_decoder_alpha": "0.18455", "t2s_enc_dec_attn_loss": "0.00195098", "s2t_ppl": "3.05", "wps": "10724.8", "ups": "0.45", "wpb": "24048.6", "bsz": "213.4", "num_updates": "4800", "lr": "1.3672e-05", "gnorm": "0.616", "clip": "0", "loss_scale": "512", "train_wall": "182", "gb_free": "20", "wall": "4565"}
2024-05-27 21:02:27 | INFO | train_inner | {"epoch": 8, "update": 7.249, "s2t_loss": "2.859", "t2s_loss": "0.52918", "loss": "1.00317", "s2t_nll_loss": "1.627", "ctc_loss": "4.091", "ce_loss": "1.128", "s2t_accuracy": "64.206", "t2s_l1_loss": "0.51027", "t2s_l2_loss": "0.24035", "t2s_bce_loss": "0.01687", "s2t_total": "7614.46", "s2t_n_correct": "4888.91", "t2s_encoder_alpha": "0.12854", "t2s_decoder_alpha": "0.18445", "t2s_enc_dec_attn_loss": "0.00204012", "s2t_ppl": "3.09", "wps": "30128.4", "ups": "1.25", "wpb": "24176.6", "bsz": "215", "num_updates": "4900", "lr": "1.3936e-05", "gnorm": "0.641", "clip": "0", "loss_scale": "512", "train_wall": "80", "gb_free": "20.9", "wall": "4645"}
2024-05-27 21:03:53 | INFO | train_inner | {"epoch": 8, "update": 7.396, "s2t_loss": "2.847", "t2s_loss": "0.52537", "loss": "1.05572", "s2t_nll_loss": "1.605", "ctc_loss": "4.09", "ce_loss": "1.112", "s2t_accuracy": "64.663", "t2s_l1_loss": "0.50699", "t2s_l2_loss": "0.23646", "t2s_bce_loss": "0.01644", "s2t_total": "8637.75", "s2t_n_correct": "5585.39", "t2s_encoder_alpha": "0.12854", "t2s_decoder_alpha": "0.18435", "t2s_enc_dec_attn_loss": "0.00195195", "s2t_ppl": "3.04", "wps": "28307.8", "ups": "1.17", "wpb": "24221.1", "bsz": "223.2", "num_updates": "5000", "lr": "1.42e-05", "gnorm": "0.643", "clip": "0", "loss_scale": "512", "train_wall": "85", "gb_free": "20.7", "wall": "4731"}
2024-05-27 21:05:17 | INFO | train_inner | {"epoch": 8, "update": 7.544, "s2t_loss": "2.852", "t2s_loss": "0.5249", "loss": "1.04409", "s2t_nll_loss": "1.614", "ctc_loss": "4.091", "ce_loss": "1.118", "s2t_accuracy": "64.509", "t2s_l1_loss": "0.50813", "t2s_l2_loss": "0.23746", "t2s_bce_loss": "0.01503", "s2t_total": "8337.58", "s2t_n_correct": "5378.47", "t2s_encoder_alpha": "0.12866", "t2s_decoder_alpha": "0.18424", "t2s_enc_dec_attn_loss": "0.00174091", "s2t_ppl": "3.06", "wps": "28790.1", "ups": "1.19", "wpb": "24155.1", "bsz": "216.4", "num_updates": "5100", "lr": "1.4464e-05", "gnorm": "0.609", "clip": "0", "loss_scale": "512", "train_wall": "83", "gb_free": "31.1", "wall": "4815"}
2024-05-27 21:06:42 | INFO | train_inner | {"epoch": 8, "update": 7.692, "s2t_loss": "2.855", "t2s_loss": "0.5212", "loss": "1.02786", "s2t_nll_loss": "1.62", "ctc_loss": "4.09", "ce_loss": "1.123", "s2t_accuracy": "64.378", "t2s_l1_loss": "0.50582", "t2s_l2_loss": "0.23495", "t2s_bce_loss": "0.01386", "s2t_total": "8205.7", "s2t_n_correct": "5282.67", "t2s_encoder_alpha": "0.12866", "t2s_decoder_alpha": "0.18413", "t2s_enc_dec_attn_loss": "0.001524", "s2t_ppl": "3.07", "wps": "28511.5", "ups": "1.18", "wpb": "24228.2", "bsz": "202.3", "num_updates": "5200", "lr": "1.4728e-05", "gnorm": "0.63", "clip": "0", "loss_scale": "512", "train_wall": "84", "gb_free": "20.9", "wall": "4900"}
2024-05-27 21:08:04 | INFO | train_inner | {"epoch": 8, "update": 7.84, "s2t_loss": "2.852", "t2s_loss": "0.52077", "loss": "1.00681", "s2t_nll_loss": "1.608", "ctc_loss": "4.096", "ce_loss": "1.115", "s2t_accuracy": "64.593", "t2s_l1_loss": "0.50398", "t2s_l2_loss": "0.23337", "t2s_bce_loss": "0.01507", "s2t_total": "7656.14", "s2t_n_correct": "4945.3", "t2s_encoder_alpha": "0.12869", "t2s_decoder_alpha": "0.18406", "t2s_enc_dec_attn_loss": "0.00172758", "s2t_ppl": "3.05", "wps": "29688.5", "ups": "1.22", "wpb": "24290.9", "bsz": "218.6", "num_updates": "5300", "lr": "1.4992e-05", "gnorm": "0.624", "clip": "0", "loss_scale": "512", "train_wall": "81", "gb_free": "19.8", "wall": "4982"}
2024-05-27 21:09:27 | INFO | train_inner | {"epoch": 8, "update": 7.988, "s2t_loss": "2.853", "t2s_loss": "0.51995", "loss": "1.04107", "s2t_nll_loss": "1.612", "ctc_loss": "4.094", "ce_loss": "1.118", "s2t_accuracy": "64.482", "t2s_l1_loss": "0.50373", "t2s_l2_loss": "0.23344", "t2s_bce_loss": "0.01463", "s2t_total": "8263.76", "s2t_n_correct": "5328.64", "t2s_encoder_alpha": "0.12876", "t2s_decoder_alpha": "0.18396", "t2s_enc_dec_attn_loss": "0.00158041", "s2t_ppl": "3.06", "wps": "29096.1", "ups": "1.2", "wpb": "24210.6", "bsz": "212.4", "num_updates": "5400", "lr": "1.5256e-05", "gnorm": "0.635", "clip": "0", "loss_scale": "512", "train_wall": "83", "gb_free": "21.8", "wall": "5065"}
2024-05-27 21:09:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 5408 updates
2024-05-27 21:09:33 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint8.pt
2024-05-27 21:09:36 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint8.pt
2024-05-27 21:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint8.pt (epoch 8 @ 5408 updates, score None) (writing took 5.848566234111786 seconds)
2024-05-27 21:09:38 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-05-27 21:09:38 | INFO | train | {"epoch": 8, "train_s2t_loss": "2.853", "train_t2s_loss": "0.5237", "train_loss": "1.02922", "train_s2t_nll_loss": "1.614", "train_ctc_loss": "4.092", "train_ce_loss": "1.119", "train_s2t_accuracy": "64.477", "train_t2s_l1_loss": "0.50661", "train_t2s_l2_loss": "0.23612", "train_t2s_bce_loss": "0.0153", "train_s2t_total": "8130.07", "train_s2t_n_correct": "5242.04", "train_t2s_encoder_alpha": "0.12862", "train_t2s_decoder_alpha": "0.18423", "train_t2s_enc_dec_attn_loss": "0.00178897", "train_s2t_ppl": "3.06", "train_wps": "23282.8", "train_ups": "0.96", "train_wpb": "24179.6", "train_bsz": "214.4", "train_num_updates": "5408", "train_lr": "1.52771e-05", "train_gnorm": "0.63", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "660", "train_gb_free": "19", "train_wall": "5076"}
2024-05-27 21:09:38 | INFO | fairseq.trainer | loading train data for epoch 9
2024-05-27 21:09:38 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 21:09:39 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 21:09:39 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 21:09:39 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 21:09:39 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 21:09:39 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 21:09:39 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 21:09:39 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 21:09:39 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 21:09:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 21:09:39 | INFO | fairseq.trainer | begin training epoch 9
2024-05-27 21:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 21:11:23 | INFO | train_inner | {"epoch": 9, "update": 8.136, "s2t_loss": "2.839", "t2s_loss": "0.51881", "loss": "1.02086", "s2t_nll_loss": "1.589", "ctc_loss": "4.09", "ce_loss": "1.101", "s2t_accuracy": "64.933", "t2s_l1_loss": "0.50237", "t2s_l2_loss": "0.23214", "t2s_bce_loss": "0.01484", "s2t_total": "8269.61", "s2t_n_correct": "5369.7", "t2s_encoder_alpha": "0.12882", "t2s_decoder_alpha": "0.18389", "t2s_enc_dec_attn_loss": "0.00159862", "s2t_ppl": "3.01", "wps": "20645.9", "ups": "0.86", "wpb": "24049.2", "bsz": "220.1", "num_updates": "5500", "lr": "1.552e-05", "gnorm": "0.594", "clip": "0", "loss_scale": "512", "train_wall": "81", "gb_free": "29.5", "wall": "5181"}
2024-05-27 21:12:45 | INFO | train_inner | {"epoch": 9, "update": 8.284, "s2t_loss": "2.844", "t2s_loss": "0.51748", "loss": "1.03199", "s2t_nll_loss": "1.6", "ctc_loss": "4.088", "ce_loss": "1.109", "s2t_accuracy": "64.758", "t2s_l1_loss": "0.50305", "t2s_l2_loss": "0.23361", "t2s_bce_loss": "0.01304", "s2t_total": "8190.21", "s2t_n_correct": "5303.85", "t2s_encoder_alpha": "0.12883", "t2s_decoder_alpha": "0.18375", "t2s_enc_dec_attn_loss": "0.00138494", "s2t_ppl": "3.03", "wps": "29762.6", "ups": "1.23", "wpb": "24159", "bsz": "200", "num_updates": "5600", "lr": "1.5784e-05", "gnorm": "0.6", "clip": "0", "loss_scale": "512", "train_wall": "81", "gb_free": "20", "wall": "5262"}
2024-05-27 21:14:08 | INFO | train_inner | {"epoch": 9, "update": 8.432, "s2t_loss": "2.843", "t2s_loss": "0.51553", "loss": "1.03394", "s2t_nll_loss": "1.595", "ctc_loss": "4.09", "ce_loss": "1.106", "s2t_accuracy": "64.842", "t2s_l1_loss": "0.49973", "t2s_l2_loss": "0.22989", "t2s_bce_loss": "0.01426", "s2t_total": "8455.04", "s2t_n_correct": "5482.44", "t2s_encoder_alpha": "0.12889", "t2s_decoder_alpha": "0.18364", "t2s_enc_dec_attn_loss": "0.00153714", "s2t_ppl": "3.02", "wps": "29043.2", "ups": "1.2", "wpb": "24174.9", "bsz": "211.9", "num_updates": "5700", "lr": "1.6048e-05", "gnorm": "0.598", "clip": "0", "loss_scale": "512", "train_wall": "83", "gb_free": "20", "wall": "5346"}
2024-05-27 21:15:31 | INFO | train_inner | {"epoch": 9, "update": 8.58, "s2t_loss": "2.839", "t2s_loss": "0.51892", "loss": "1.01162", "s2t_nll_loss": "1.591", "ctc_loss": "4.087", "ce_loss": "1.103", "s2t_accuracy": "64.901", "t2s_l1_loss": "0.50193", "t2s_l2_loss": "0.23271", "t2s_bce_loss": "0.01536", "s2t_total": "7695.12", "s2t_n_correct": "4994.19", "t2s_encoder_alpha": "0.12894", "t2s_decoder_alpha": "0.18348", "t2s_enc_dec_attn_loss": "0.0016334", "s2t_ppl": "3.01", "wps": "28886.3", "ups": "1.2", "wpb": "24147", "bsz": "206.3", "num_updates": "5800", "lr": "1.6312e-05", "gnorm": "0.62", "clip": "0", "loss_scale": "512", "train_wall": "83", "gb_free": "20", "wall": "5429"}
2024-05-27 21:16:56 | INFO | train_inner | {"epoch": 9, "update": 8.728, "s2t_loss": "2.841", "t2s_loss": "0.51708", "loss": "1.04003", "s2t_nll_loss": "1.593", "ctc_loss": "4.089", "ce_loss": "1.104", "s2t_accuracy": "64.852", "t2s_l1_loss": "0.50058", "t2s_l2_loss": "0.2303", "t2s_bce_loss": "0.01499", "s2t_total": "8241.94", "s2t_n_correct": "5345.06", "t2s_encoder_alpha": "0.12897", "t2s_decoder_alpha": "0.18347", "t2s_enc_dec_attn_loss": "0.00151224", "s2t_ppl": "3.02", "wps": "28499.7", "ups": "1.18", "wpb": "24223.4", "bsz": "218.4", "num_updates": "5900", "lr": "1.6576e-05", "gnorm": "0.662", "clip": "0", "loss_scale": "512", "train_wall": "85", "gb_free": "31", "wall": "5514"}
2024-05-27 21:18:17 | INFO | train_inner | {"epoch": 9, "update": 8.876, "s2t_loss": "2.858", "t2s_loss": "0.51687", "loss": "0.9891", "s2t_nll_loss": "1.613", "ctc_loss": "4.103", "ce_loss": "1.118", "s2t_accuracy": "64.452", "t2s_l1_loss": "0.49996", "t2s_l2_loss": "0.23063", "t2s_bce_loss": "0.0153", "s2t_total": "7474.83", "s2t_n_correct": "4817.67", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.18333", "t2s_enc_dec_attn_loss": "0.0016061", "s2t_ppl": "3.06", "wps": "30090.5", "ups": "1.24", "wpb": "24307.5", "bsz": "229.8", "num_updates": "6000", "lr": "1.684e-05", "gnorm": "0.644", "clip": "0", "loss_scale": "512", "train_wall": "80", "gb_free": "19.9", "wall": "5595"}
2024-05-27 21:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 6084 updates
2024-05-27 21:19:27 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint9.pt
2024-05-27 21:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint9.pt
2024-05-27 21:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint9.pt (epoch 9 @ 6084 updates, score None) (writing took 5.986347817350179 seconds)
2024-05-27 21:19:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-05-27 21:19:33 | INFO | train | {"epoch": 9, "train_s2t_loss": "2.845", "train_t2s_loss": "0.51661", "train_loss": "1.02266", "train_s2t_nll_loss": "1.596", "train_ctc_loss": "4.094", "train_ce_loss": "1.106", "train_s2t_accuracy": "64.817", "train_t2s_l1_loss": "0.50055", "train_t2s_l2_loss": "0.23093", "train_t2s_bce_loss": "0.01453", "train_s2t_total": "8092.95", "train_s2t_n_correct": "5245.59", "train_t2s_encoder_alpha": "0.12893", "train_t2s_decoder_alpha": "0.18354", "train_t2s_enc_dec_attn_loss": "0.00153097", "train_s2t_ppl": "3.02", "train_wps": "27486.7", "train_ups": "1.14", "train_wpb": "24179.5", "train_bsz": "214.4", "train_num_updates": "6084", "train_lr": "1.70618e-05", "train_gnorm": "0.614", "train_clip": "0", "train_loss_scale": "512", "train_train_wall": "556", "train_gb_free": "21.9", "train_wall": "5671"}
2024-05-27 21:19:33 | INFO | fairseq.trainer | loading train data for epoch 10
2024-05-27 21:19:33 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 21:19:33 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 21:19:33 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 21:19:33 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 21:19:33 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 21:19:33 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 21:19:33 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 21:19:34 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 21:19:34 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 21:19:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 21:19:34 | INFO | fairseq.trainer | begin training epoch 10
2024-05-27 21:19:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 21:20:17 | INFO | train_inner | {"epoch": 10, "update": 9.024, "s2t_loss": "2.848", "t2s_loss": "0.51073", "loss": "1.03072", "s2t_nll_loss": "1.588", "ctc_loss": "4.109", "ce_loss": "1.1", "s2t_accuracy": "64.991", "t2s_l1_loss": "0.49578", "t2s_l2_loss": "0.22685", "t2s_bce_loss": "0.01355", "s2t_total": "8332.26", "s2t_n_correct": "5415.23", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.18321", "t2s_enc_dec_attn_loss": "0.00139926", "s2t_ppl": "3.01", "wps": "20016.1", "ups": "0.83", "wpb": "23989.2", "bsz": "209.4", "num_updates": "6100", "lr": "1.7104e-05", "gnorm": "0.593", "clip": "0", "loss_scale": "512", "train_wall": "83", "gb_free": "22.9", "wall": "5715"}
2024-05-27 21:21:40 | INFO | train_inner | {"epoch": 10, "update": 9.172, "s2t_loss": "2.839", "t2s_loss": "0.51176", "loss": "1.01413", "s2t_nll_loss": "1.589", "ctc_loss": "4.089", "ce_loss": "1.102", "s2t_accuracy": "64.985", "t2s_l1_loss": "0.49668", "t2s_l2_loss": "0.22712", "t2s_bce_loss": "0.01381", "s2t_total": "8157.53", "s2t_n_correct": "5301.2", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.18307", "t2s_enc_dec_attn_loss": "0.00128436", "s2t_ppl": "3.01", "wps": "29319.4", "ups": "1.21", "wpb": "24251.8", "bsz": "211.1", "num_updates": "6200", "lr": "1.7368e-05", "gnorm": "0.64", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "19.3", "wall": "5798"}
2024-05-27 21:23:04 | INFO | train_inner | {"epoch": 10, "update": 9.32, "s2t_loss": "2.839", "t2s_loss": "0.50996", "loss": "1.03678", "s2t_nll_loss": "1.59", "ctc_loss": "4.088", "ce_loss": "1.102", "s2t_accuracy": "64.952", "t2s_l1_loss": "0.49388", "t2s_l2_loss": "0.22464", "t2s_bce_loss": "0.01458", "s2t_total": "8166.34", "s2t_n_correct": "5304.18", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.18289", "t2s_enc_dec_attn_loss": "0.00150405", "s2t_ppl": "3.01", "wps": "28682.3", "ups": "1.18", "wpb": "24229.1", "bsz": "227.3", "num_updates": "6300", "lr": "1.7632e-05", "gnorm": "0.588", "clip": "0", "loss_scale": "1024", "train_wall": "84", "gb_free": "20.8", "wall": "5882"}
2024-05-27 21:24:26 | INFO | train_inner | {"epoch": 10, "update": 9.467, "s2t_loss": "2.841", "t2s_loss": "0.50772", "loss": "1.01545", "s2t_nll_loss": "1.597", "ctc_loss": "4.084", "ce_loss": "1.107", "s2t_accuracy": "64.782", "t2s_l1_loss": "0.4923", "t2s_l2_loss": "0.2231", "t2s_bce_loss": "0.01395", "s2t_total": "8013.7", "s2t_n_correct": "5191.42", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.1827", "t2s_enc_dec_attn_loss": "0.00147588", "s2t_ppl": "3.03", "wps": "29568.7", "ups": "1.22", "wpb": "24139.3", "bsz": "212.3", "num_updates": "6400", "lr": "1.7896e-05", "gnorm": "0.606", "clip": "0", "loss_scale": "1024", "train_wall": "81", "gb_free": "19.6", "wall": "5964"}
2024-05-27 21:25:48 | INFO | train_inner | {"epoch": 10, "update": 9.615, "s2t_loss": "2.839", "t2s_loss": "0.50302", "loss": "0.98643", "s2t_nll_loss": "1.596", "ctc_loss": "4.083", "ce_loss": "1.106", "s2t_accuracy": "64.797", "t2s_l1_loss": "0.48833", "t2s_l2_loss": "0.22003", "t2s_bce_loss": "0.01339", "s2t_total": "8213.65", "s2t_n_correct": "5322.16", "t2s_encoder_alpha": "0.12903", "t2s_decoder_alpha": "0.18251", "t2s_enc_dec_attn_loss": "0.00130603", "s2t_ppl": "3.02", "wps": "29562.7", "ups": "1.22", "wpb": "24253", "bsz": "214", "num_updates": "6500", "lr": "1.816e-05", "gnorm": "0.638", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "30.5", "wall": "6046"}
2024-05-27 21:27:13 | INFO | train_inner | {"epoch": 10, "update": 9.763, "s2t_loss": "2.832", "t2s_loss": "0.50031", "loss": "1.02714", "s2t_nll_loss": "1.584", "ctc_loss": "4.081", "ce_loss": "1.098", "s2t_accuracy": "65.01", "t2s_l1_loss": "0.48552", "t2s_l2_loss": "0.21688", "t2s_bce_loss": "0.01343", "s2t_total": "8504.79", "s2t_n_correct": "5529", "t2s_encoder_alpha": "0.12893", "t2s_decoder_alpha": "0.18237", "t2s_enc_dec_attn_loss": "0.00135362", "s2t_ppl": "3", "wps": "28370.9", "ups": "1.17", "wpb": "24223.8", "bsz": "218.4", "num_updates": "6600", "lr": "1.8424e-05", "gnorm": "0.639", "clip": "0", "loss_scale": "1024", "train_wall": "85", "gb_free": "19.6", "wall": "6131"}
2024-05-27 21:28:35 | INFO | train_inner | {"epoch": 10, "update": 9.911, "s2t_loss": "2.83", "t2s_loss": "0.49481", "loss": "0.99719", "s2t_nll_loss": "1.585", "ctc_loss": "4.075", "ce_loss": "1.099", "s2t_accuracy": "65.025", "t2s_l1_loss": "0.48181", "t2s_l2_loss": "0.21386", "t2s_bce_loss": "0.01179", "s2t_total": "8262.3", "s2t_n_correct": "5372.56", "t2s_encoder_alpha": "0.12891", "t2s_decoder_alpha": "0.18214", "t2s_enc_dec_attn_loss": "0.00121128", "s2t_ppl": "3", "wps": "29478.3", "ups": "1.22", "wpb": "24176.4", "bsz": "205.8", "num_updates": "6700", "lr": "1.8688e-05", "gnorm": "0.596", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "19.5", "wall": "6213"}
2024-05-27 21:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 6760 updates
2024-05-27 21:29:23 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint10.pt
2024-05-27 21:29:27 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint10.pt
2024-05-27 21:29:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint10.pt (epoch 10 @ 6760 updates, score None) (writing took 5.874349156394601 seconds)
2024-05-27 21:29:29 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-05-27 21:29:29 | INFO | train | {"epoch": 10, "train_s2t_loss": "2.836", "train_t2s_loss": "0.50415", "train_loss": "1.0123", "train_s2t_nll_loss": "1.589", "train_ctc_loss": "4.082", "train_ce_loss": "1.102", "train_s2t_accuracy": "64.941", "train_t2s_l1_loss": "0.48911", "train_t2s_l2_loss": "0.22051", "train_t2s_bce_loss": "0.01367", "train_s2t_total": "8192.71", "train_s2t_n_correct": "5320.43", "train_t2s_encoder_alpha": "0.12898", "train_t2s_decoder_alpha": "0.18257", "train_t2s_enc_dec_attn_loss": "0.00137119", "train_s2t_ppl": "3.01", "train_wps": "27423.7", "train_ups": "1.13", "train_wpb": "24179.9", "train_bsz": "214.5", "train_num_updates": "6760", "train_lr": "1.88464e-05", "train_gnorm": "0.629", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "556", "train_gb_free": "28.4", "train_wall": "6267"}
2024-05-27 21:29:29 | INFO | fairseq.trainer | loading train data for epoch 11
2024-05-27 21:29:29 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 21:29:29 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 21:29:29 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 21:29:29 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 21:29:29 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 21:29:29 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 21:29:30 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 21:29:30 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 21:29:30 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 21:29:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 21:29:30 | INFO | fairseq.trainer | begin training epoch 11
2024-05-27 21:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 21:30:33 | INFO | train_inner | {"epoch": 11, "update": 10.059, "s2t_loss": "2.821", "t2s_loss": "0.49619", "loss": "0.98872", "s2t_nll_loss": "1.578", "ctc_loss": "4.063", "ce_loss": "1.094", "s2t_accuracy": "65.144", "t2s_l1_loss": "0.48001", "t2s_l2_loss": "0.21302", "t2s_bce_loss": "0.0147", "s2t_total": "7805.23", "s2t_n_correct": "5084.63", "t2s_encoder_alpha": "0.12887", "t2s_decoder_alpha": "0.18198", "t2s_enc_dec_attn_loss": "0.00147909", "s2t_ppl": "2.99", "wps": "20415", "ups": "0.85", "wpb": "24003.9", "bsz": "216.2", "num_updates": "6800", "lr": "1.8952e-05", "gnorm": "0.791", "clip": "0", "loss_scale": "1024", "train_wall": "80", "gb_free": "19.9", "wall": "6331"}
2024-05-27 21:33:15 | INFO | train_inner | {"epoch": 11, "update": 10.207, "s2t_loss": "2.786", "t2s_loss": "0.49031", "loss": "0.98214", "s2t_nll_loss": "1.579", "ctc_loss": "3.994", "ce_loss": "1.094", "s2t_accuracy": "65.162", "t2s_l1_loss": "0.47649", "t2s_l2_loss": "0.20879", "t2s_bce_loss": "0.01241", "s2t_total": "7884.44", "s2t_n_correct": "5137.66", "t2s_encoder_alpha": "0.12878", "t2s_decoder_alpha": "0.18178", "t2s_enc_dec_attn_loss": "0.00141586", "s2t_ppl": "2.99", "wps": "14951.6", "ups": "0.62", "wpb": "24256.5", "bsz": "221.8", "num_updates": "6900", "lr": "1.9216e-05", "gnorm": "0.563", "clip": "0", "loss_scale": "1024", "train_wall": "162", "gb_free": "22.8", "wall": "6493"}
2024-05-27 21:34:36 | INFO | train_inner | {"epoch": 11, "update": 10.355, "s2t_loss": "2.72", "t2s_loss": "0.48551", "loss": "0.93712", "s2t_nll_loss": "1.577", "ctc_loss": "3.864", "ce_loss": "1.093", "s2t_accuracy": "65.158", "t2s_l1_loss": "0.47344", "t2s_l2_loss": "0.20648", "t2s_bce_loss": "0.01089", "s2t_total": "7691.77", "s2t_n_correct": "5011.77", "t2s_encoder_alpha": "0.12878", "t2s_decoder_alpha": "0.18167", "t2s_enc_dec_attn_loss": "0.00118386", "s2t_ppl": "2.98", "wps": "29893.4", "ups": "1.23", "wpb": "24262.7", "bsz": "210.1", "num_updates": "7000", "lr": "1.948e-05", "gnorm": "0.629", "clip": "0", "loss_scale": "1024", "train_wall": "81", "gb_free": "24.7", "wall": "6574"}
2024-05-27 21:35:58 | INFO | train_inner | {"epoch": 11, "update": 10.503, "s2t_loss": "2.612", "t2s_loss": "0.48851", "loss": "0.91314", "s2t_nll_loss": "1.588", "ctc_loss": "3.635", "ce_loss": "1.1", "s2t_accuracy": "64.983", "t2s_l1_loss": "0.47527", "t2s_l2_loss": "0.20811", "t2s_bce_loss": "0.01191", "s2t_total": "7599.41", "s2t_n_correct": "4938.35", "t2s_encoder_alpha": "0.12878", "t2s_decoder_alpha": "0.1816", "t2s_enc_dec_attn_loss": "0.00132515", "s2t_ppl": "3.01", "wps": "29729.9", "ups": "1.22", "wpb": "24310.7", "bsz": "220.1", "num_updates": "7100", "lr": "1.9744e-05", "gnorm": "0.584", "clip": "0", "loss_scale": "1024", "train_wall": "81", "gb_free": "19.9", "wall": "6656"}
2024-05-27 21:37:23 | INFO | train_inner | {"epoch": 11, "update": 10.651, "s2t_loss": "2.428", "t2s_loss": "0.48824", "loss": "0.94674", "s2t_nll_loss": "1.578", "ctc_loss": "3.278", "ce_loss": "1.094", "s2t_accuracy": "65.153", "t2s_l1_loss": "0.47465", "t2s_l2_loss": "0.20688", "t2s_bce_loss": "0.0122", "s2t_total": "8840.82", "s2t_n_correct": "5760.08", "t2s_encoder_alpha": "0.12889", "t2s_decoder_alpha": "0.1814", "t2s_enc_dec_attn_loss": "0.00139707", "s2t_ppl": "2.99", "wps": "28304.3", "ups": "1.18", "wpb": "24072.8", "bsz": "213.8", "num_updates": "7200", "lr": "2.0008e-05", "gnorm": "0.581", "clip": "0", "loss_scale": "1024", "train_wall": "85", "gb_free": "22.3", "wall": "6741"}
2024-05-27 21:38:46 | INFO | train_inner | {"epoch": 11, "update": 10.799, "s2t_loss": "2.224", "t2s_loss": "0.48378", "loss": "0.86454", "s2t_nll_loss": "1.576", "ctc_loss": "2.871", "ce_loss": "1.093", "s2t_accuracy": "65.206", "t2s_l1_loss": "0.47019", "t2s_l2_loss": "0.20324", "t2s_bce_loss": "0.01217", "s2t_total": "8503.61", "s2t_n_correct": "5544.85", "t2s_encoder_alpha": "0.12892", "t2s_decoder_alpha": "0.18131", "t2s_enc_dec_attn_loss": "0.00141944", "s2t_ppl": "2.98", "wps": "29356.4", "ups": "1.21", "wpb": "24207.9", "bsz": "217.5", "num_updates": "7300", "lr": "2.0272e-05", "gnorm": "0.557", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "19.7", "wall": "6824"}
2024-05-27 21:40:09 | INFO | train_inner | {"epoch": 11, "update": 10.947, "s2t_loss": "2.04", "t2s_loss": "0.48773", "loss": "0.80865", "s2t_nll_loss": "1.57", "ctc_loss": "2.511", "ce_loss": "1.088", "s2t_accuracy": "65.359", "t2s_l1_loss": "0.47237", "t2s_l2_loss": "0.20587", "t2s_bce_loss": "0.01392", "s2t_total": "8079.57", "s2t_n_correct": "5280.71", "t2s_encoder_alpha": "0.12887", "t2s_decoder_alpha": "0.18113", "t2s_enc_dec_attn_loss": "0.00144011", "s2t_ppl": "2.97", "wps": "28884", "ups": "1.2", "wpb": "24142.8", "bsz": "211.1", "num_updates": "7400", "lr": "2.0536e-05", "gnorm": "0.721", "clip": "0", "loss_scale": "1024", "train_wall": "83", "gb_free": "18.3", "wall": "6907"}
2024-05-27 21:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 7436 updates
2024-05-27 21:40:40 | INFO | fairseq.trainer | Saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint11.pt
2024-05-27 21:40:44 | INFO | fairseq.trainer | Finished saving checkpoint to /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint11.pt
2024-05-27 21:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/results/sttatts/models_english_150K_t5_new/checkpoint11.pt (epoch 11 @ 7436 updates, score None) (writing took 6.1891940901987255 seconds)
2024-05-27 21:40:47 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-05-27 21:40:47 | INFO | train | {"epoch": 11, "train_s2t_loss": "2.453", "train_t2s_loss": "0.48731", "train_loss": "0.90913", "train_s2t_nll_loss": "1.581", "train_ctc_loss": "3.325", "train_ce_loss": "1.096", "train_s2t_accuracy": "65.107", "train_t2s_l1_loss": "0.47377", "train_t2s_l2_loss": "0.20659", "train_t2s_bce_loss": "0.01219", "train_s2t_total": "8117.66", "train_s2t_n_correct": "5285.16", "train_t2s_encoder_alpha": "0.12885", "train_t2s_decoder_alpha": "0.18149", "train_t2s_enc_dec_attn_loss": "0.00135005", "train_s2t_ppl": "2.99", "train_wps": "24132.1", "train_ups": "1", "train_wpb": "24179.6", "train_bsz": "214.5", "train_num_updates": "7436", "train_lr": "2.0631e-05", "train_gnorm": "0.626", "train_clip": "0", "train_loss_scale": "1024", "train_train_wall": "637", "train_gb_free": "22.2", "train_wall": "6945"}
2024-05-27 21:40:47 | INFO | fairseq.trainer | loading train data for epoch 12
2024-05-27 21:40:47 | INFO | artst.tasks.artst | tokenizer: /fsx/hyperpod-input-datasets/AROA6GBMFKRI2VWQAUGYI:Hawau.Toyin@mbzuai.ac.ae/sttatts_en/spm_char.model
2024-05-27 21:40:47 | INFO | artst.data.speech_to_text_dataset | max_keep=480256, min_keep=1056, loaded 28539, skipped 0 short and 0 long, longest-loaded=392400, shortest-loaded=22560
2024-05-27 21:40:47 | INFO | artst.data.speech_to_text_dataset | normalize=False
2024-05-27 21:40:47 | INFO | artst.data.text_to_speech_dataset | max_keep=480256, min_keep=None, loaded 116460, skipped 0 short and 40 long, longest-loaded=479680, shortest-loaded=1760
2024-05-27 21:40:47 | INFO | artst.data.text_to_speech_dataset | reduction_factor=2, normalize=False
2024-05-27 21:40:47 | INFO | artst.tasks.artst | Task: multitask, Loaded 28539 samples of speech-to-text_dataset
sample_ratios=[0.5, 0.5]
batch_ratio=[0.5, 0.5]
Do nothing
Do nothing
2024-05-27 21:40:47 | INFO | artst.data.multitask_dataset | update dataset size
2024-05-27 21:40:47 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 1878 for dataset 0
2024-05-27 21:40:47 | INFO | artst.data.multitask_dataset | Adjust batch by ratio 1 and the number of batch is 3525 for dataset 1
2024-05-27 21:40:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 676
2024-05-27 21:40:47 | INFO | fairseq.trainer | begin training epoch 12
2024-05-27 21:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-05-27 21:42:13 | INFO | train_inner | {"epoch": 12, "update": 11.095, "s2t_loss": "1.923", "t2s_loss": "0.48275", "loss": "0.77417", "s2t_nll_loss": "1.585", "ctc_loss": "2.262", "ce_loss": "1.099", "s2t_accuracy": "65.09", "t2s_l1_loss": "0.46994", "t2s_l2_loss": "0.20342", "t2s_bce_loss": "0.01147", "s2t_total": "7868.18", "s2t_n_correct": "5121.42", "t2s_encoder_alpha": "0.1289", "t2s_decoder_alpha": "0.18092", "t2s_enc_dec_attn_loss": "0.00132842", "s2t_ppl": "3", "wps": "19404", "ups": "0.81", "wpb": "24057.1", "bsz": "210", "num_updates": "7500", "lr": "2.08e-05", "gnorm": "0.642", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "19.7", "wall": "7031"}
2024-05-27 22:14:19 | INFO | train_inner | {"epoch": 12, "update": 11.243, "s2t_loss": "1.782", "t2s_loss": "0.48239", "loss": "0.76691", "s2t_nll_loss": "1.553", "ctc_loss": "2.012", "ce_loss": "1.076", "s2t_accuracy": "65.686", "t2s_l1_loss": "0.46924", "t2s_l2_loss": "0.20245", "t2s_bce_loss": "0.01179", "s2t_total": "8902.86", "s2t_n_correct": "5847.92", "t2s_encoder_alpha": "0.12879", "t2s_decoder_alpha": "0.1808", "t2s_enc_dec_attn_loss": "0.00136504", "s2t_ppl": "2.93", "wps": "1250.6", "ups": "0.05", "wpb": "24083.8", "bsz": "213.2", "num_updates": "7600", "lr": "2.1064e-05", "gnorm": "0.674", "clip": "0", "loss_scale": "1024", "train_wall": "1137", "gb_free": "20.4", "wall": "8957"}
2024-05-27 22:17:03 | INFO | train_inner | {"epoch": 12, "update": 11.391, "s2t_loss": "1.676", "t2s_loss": "0.48169", "loss": "0.72477", "s2t_nll_loss": "1.562", "ctc_loss": "1.789", "ce_loss": "1.083", "s2t_accuracy": "65.424", "t2s_l1_loss": "0.46869", "t2s_l2_loss": "0.20217", "t2s_bce_loss": "0.01174", "s2t_total": "8489.89", "s2t_n_correct": "5554.41", "t2s_encoder_alpha": "0.1288", "t2s_decoder_alpha": "0.18073", "t2s_enc_dec_attn_loss": "0.00125148", "s2t_ppl": "2.95", "wps": "14734.9", "ups": "0.61", "wpb": "24209.4", "bsz": "212.5", "num_updates": "7700", "lr": "2.1328e-05", "gnorm": "0.689", "clip": "0", "loss_scale": "1024", "train_wall": "164", "gb_free": "20.6", "wall": "9121"}
2024-05-27 22:18:29 | INFO | train_inner | {"epoch": 12, "update": 11.538, "s2t_loss": "1.584", "t2s_loss": "0.48007", "loss": "0.70084", "s2t_nll_loss": "1.55", "ctc_loss": "1.617", "ce_loss": "1.074", "s2t_accuracy": "65.738", "t2s_l1_loss": "0.46777", "t2s_l2_loss": "0.20122", "t2s_bce_loss": "0.0111", "s2t_total": "8163.15", "s2t_n_correct": "5366.32", "t2s_encoder_alpha": "0.12878", "t2s_decoder_alpha": "0.18066", "t2s_enc_dec_attn_loss": "0.00120115", "s2t_ppl": "2.93", "wps": "28398.7", "ups": "1.17", "wpb": "24255.9", "bsz": "214.6", "num_updates": "7800", "lr": "2.1592e-05", "gnorm": "0.588", "clip": "0", "loss_scale": "1024", "train_wall": "85", "gb_free": "20.4", "wall": "9207"}
2024-05-27 22:19:51 | INFO | train_inner | {"epoch": 12, "update": 11.686, "s2t_loss": "1.553", "t2s_loss": "0.47763", "loss": "0.68655", "s2t_nll_loss": "1.551", "ctc_loss": "1.556", "ce_loss": "1.075", "s2t_accuracy": "65.764", "t2s_l1_loss": "0.465", "t2s_l2_loss": "0.19852", "t2s_bce_loss": "0.01136", "s2t_total": "8242.57", "s2t_n_correct": "5420.66", "t2s_encoder_alpha": "0.12878", "t2s_decoder_alpha": "0.18048", "t2s_enc_dec_attn_loss": "0.0012706", "s2t_ppl": "2.93", "wps": "29525.9", "ups": "1.22", "wpb": "24278.5", "bsz": "216", "num_updates": "7900", "lr": "2.1856e-05", "gnorm": "0.689", "clip": "0", "loss_scale": "1024", "train_wall": "82", "gb_free": "19.8", "wall": "9289"}
